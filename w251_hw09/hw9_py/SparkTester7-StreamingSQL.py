from pyspark import SparkContext
from pyspark.streaming import StreamingContext
import json

'''
Test for Spark Stream to be used with a Tweet Tester, to see if we can using SQL to parse the tweets from the twitter stream
Consumes the tweets sampled by the Tweet Tester app.

On Terminal 1 run,
$ python SparkTester7-StreamingSQL.py

then start typing into terminal 1, hit enter to send a line of text

On Terminal 2 run,
$ python SparkTester7-StreamingSQL.py

as the tweet tester streams tweets into the spark streaming app, we should see ...


See 
* https://spark.apache.org/docs/latest/streaming-programming-guide.html
* https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/streaming/sql_network_wordcount.py
* https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html
* https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html
'''

my_port = 5555
my_host = "spark-2-1"
my_batch_interval = 1 #batch interval of seconds
my_app_name = "PythonSqlNetworkWordCount"
my_spark_master = "local[2]" #local StreamingContext with two working threads


import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import Row, SparkSession


def getSparkSessionInstance(sparkConf):
    if ('sparkSessionSingletonInstance' not in globals()):
        globals()['sparkSessionSingletonInstance'] = SparkSession\
            .builder\
            .config(conf=sparkConf)\
            .getOrCreate()
    return globals()['sparkSessionSingletonInstance']

    
def main():
    host, port = my_host, my_port
    sc = SparkContext(appName="PythonSqlNetworkWordCount")
    ssc = StreamingContext(sc, my_batch_interval)

    # Create a socket stream on target ip:port and count the
    # words in input stream of \n delimited text (eg. generated by 'nc')
    tweets = ssc.socketTextStream(host, int(port))

    # Convert RDDs of the words DStream to DataFrame and run SQL query
    def process(time, rdd):
        print("========= %s =========" % str(time))
        try:
            # Get the singleton instance of SparkSession
            spark = getSparkSessionInstance(rdd.context.getConf())
            # Convert RDD[String] to RDD[Row] to DataFrame
            rowRdd = rdd.map(lambda t: Row(tweet=t))
            tweetsDataFrame = spark.createDataFrame(rowRdd)
            # Creates a temporary view using the DataFrame.
            tweetsDataFrame.createOrReplaceTempView("tweets")
            # Do word count on table using SQL and print it
            tweetCountsDataFrame = spark.sql("select tweet.id from tweets")
            tweetCountsDataFrame.show()
        except:
            pass

    tweets.foreachRDD(process)
    ssc.start()
    ssc.awaitTermination()

    
if __name__ == "__main__":
    main()